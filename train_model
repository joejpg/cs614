import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction
import torch
from torch.utils.data import Dataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

df = pd.read_csv("preprocessed_movie_corpus.csv")
class MovieCorpusDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')
        encoding = {key: val.squeeze() for key, val in encoding.items()}
        encoding['labels'] = torch.tensor(label)
        return encoding

texts = df['Text'].tolist()
labels = [0] * len(texts)

train_texts, eval_texts, train_labels, eval_labels = train_test_split(texts, labels, test_size=0.2)

train_dataset = MovieCorpusDataset(train_texts, train_labels)
eval_dataset = MovieCorpusDataset(eval_texts, eval_labels)

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",  
)

def compute_metrics(p: EvalPrediction):
    try:
        preds = np.argmax(p.predictions, axis=1)
        precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')
        acc = accuracy_score(p.label_ids, preds)
        return {
            'accuracy': acc,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    except Exception as e:
        print(f"Error in compute_metrics: {e}")
        raise

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

#Evaluate
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")

#Save the model
trainer.save_model('trained_model')

with open("Bert-NLP/train_model.py", "w") as file:
    file.write(train_model_script)
